{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluating Models\n",
    "*by Evgeny Sushko*\n",
    "<img width=\"50%\" height=\"50%\" src=\"http://i.piccy.info/i9/666d78be04fbcf04fdb321d5953d1fa5/1492256847/123248/1137898/ua_parrots.jpg\">\n",
    "\n",
    "---\n",
    "## <a name=\"0\"></a> Table of Contents:\n",
    "1. [Model evaluation applications](#1)    \n",
    "    1. [Generalization performance](#1.1)\n",
    "    2. [Model selection](#1.2)\n",
    "    3. [Algorithm selection](#1.3)   \n",
    "2. [Model evaluation techniques](#2)\n",
    "   1. [Holdout method (simple train/test split)](#2.1)\n",
    "       1. [exercise 1](#2.1.1)\n",
    "   2. [K-fold cross-validation](#2.2)   \n",
    "3. [Classification metrics](#3)\n",
    "   1. [Accuracy](#3.1)\n",
    "   2. [Confusion matrix](#3.2)\n",
    "   3. [Precision & Recall](#3.3)\n",
    "       1. [exercise 2](#3.3.1)\n",
    "   4. [F-1 score](#3.4)\n",
    "   5. [Classification report](#3.5)\n",
    "4. [Appropriate merics choice](#4)\n",
    "5. [Summary](#5)\n",
    "---\n",
    "### Requirements\n",
    "1. Python 3.x (or Anaconda3 for Python 3.5, https://www.continuum.io/downloads)\n",
    "2. Scikit-learn 0.18.x (pip install scikit-learn==0.18.1, http://scikit-learn.org/)\n",
    "3. Pandas latest (http://pandas.pydata.org/)\n",
    "4. For datasets more than 1M reviews min Hardware Requirements (SDRAM >= 8 GB)\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Model evaluation is not just the end point of our machine learning pipeline. Before we handle any data, we want to plan ahead and use techniques and metrics that are suited for our purposes.\n",
    "\n",
    "In this tutorial we will go over some of these techniques and metrics and we will see how they fit into a typical machine learning workflow.\n",
    "\n",
    "### <a name=\"1\"></a> 1. Model Evaluation Applications\n",
    "Let's start with a question: **\"Why do we care about performance estimates at all?\"**\n",
    "\n",
    "<a name=\"1.1\"></a>**Generalization performance** - We want to estimate the predictive performance of our model on future (unseen) data.\n",
    "- Ideally, the estimated performance of a model tells how well it performs on unseen data – making predictions on future data is often the main problem we want to solve.\n",
    "\n",
    "<a name=\"1.2\"></a>**Model selection** - We want to increase the predictive performance by tweaking the learning algorithm and selecting the best performing model from a given hypothesis space.\n",
    "- Typically, machine learning involves a lot of experimentation. Running a learning algorithm over a training dataset with different hyperparameter settings and different features will result in different models. Since we are typically interested in selecting the best-performing model from this set, we need to find a way to estimate their respective performances in order to rank them against each other.\n",
    "\n",
    "<a name=\"1.3\"></a>**Algorithm selection** - We want to compare different ML algorithms, selecting the best-performing one.\n",
    "- We are usually not only experimenting with the one single algorithm that we think would be the “best solution” under the given circumstances. More often than not, we want to compare different algorithms to each other, oftentimes in terms of predictive and computational performance.\n",
    "\n",
    "Although these three sub-tasks have all in common that we want to estimate the performance of a model, they all require different approaches. \n",
    "\n",
    "This tutorial will focus on **supervised learning**, a subcategory of machine learning where our target values are known in our available dataset. Although many concepts also apply to regression analysis, we will focus on **classification**, the assignment of categorical target labels to the samples.\n",
    "\n",
    "[To the table of contents](#0)\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <a name=\"2\"></a>2. Model Evaluation Techniques\n",
    "#### <a name=\"2.1\"></a>Holdout method (simple train/test split)\n",
    "The holdout method is the simplest model evaluation technique. We take our labeled dataset and split it randomly into two parts: A **training set** and a **test set**\n",
    "<img src=\"https://sebastianraschka.com/images/blog/2016/model-evaluation-selection-part1/testing_01.png\" width=\"500\">\n",
    "Then, we fit a model to the training data and predict the labels of the test set.\n",
    "<img src=\"https://sebastianraschka.com/images/blog/2016/model-evaluation-selection-part1/testing_02.png\" width=\"500\">\n",
    "And the fraction of correct predictions constitutes our estimate of the prediction accuracy.\n",
    "<img src=\"https://sebastianraschka.com/images/blog/2016/model-evaluation-selection-part1/testing_03.png\" width=\"500\">\n",
    "We really don’t want to train and evaluate our model on the same training dataset, since it would introduce **overfitting**. In other words, we can’t tell whether the model simply memorized the training data or not, or whether it generalizes well to new, unseen data.\n",
    "\n",
    "##### Pros:\n",
    "    + Simple\n",
    "    + Fast\n",
    "\n",
    "##### Cons:\n",
    "    - Not so precise estimate of out-of-sample performance comparing to more advanced techniques\n",
    "\n",
    "[To the table of contents](#0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(152610, 2)"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# import data\n",
    "import pandas as pd\n",
    "\n",
    "df = pd.read_csv('../data/movie_reviews.csv')\n",
    "\n",
    "# check number of rows & columns\n",
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# split dataset to Train and Test parts\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X, y = df.text, df.label\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y,\n",
    "                                                    test_size=0.2,\n",
    "                                                    random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 1min 7s, sys: 1.31 s, total: 1min 8s\n",
      "Wall time: 46.6 s\n"
     ]
    }
   ],
   "source": [
    "# fit a model to the training data\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "vectorizer = CountVectorizer(binary=True)\n",
    "classifier = LogisticRegression(class_weight='balanced')\n",
    "\n",
    "pipeline = Pipeline([('vectorizer', vectorizer),\n",
    "                     ('classifier', classifier)])\n",
    "\n",
    "%time model = pipeline.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 3.78 s, sys: 93 ms, total: 3.87 s\n",
      "Wall time: 3.81 s\n"
     ]
    }
   ],
   "source": [
    "# predict the labels of the test set\n",
    "%time y_pred = model.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.819343424415\n"
     ]
    }
   ],
   "source": [
    "# compute prediction accuracy\n",
    "from sklearn import metrics\n",
    "\n",
    "print(\"Accuracy:\", metrics.accuracy_score(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### <a name=\"2.1.1\"></a>Excercise 1: Holdout validation\n",
    "- Split dataset to train and test.\n",
    "- Hold 30% of the data as a test part.\n",
    "- Check accuracy on the test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# my_X, my_y = df.text, df.label\n",
    "\n",
    "#### YOUR CODE STARTS HERE ###\n",
    "\n",
    "# my_X_train, my_X_test, my_y_train, my_y_test = ...\n",
    "\n",
    "#### YOUR CODE ENDS HERE ###\n",
    "\n",
    "# vectorizer = CountVectorizer(binary=True)\n",
    "# classifier = LogisticRegression(class_weight='balanced')\n",
    "# my_pipeline = Pipeline([('vectorizer', vectorizer),\n",
    "#                      ('classifier', classifier)])\n",
    "# my_model = my_pipeline.fit(my_X_train, my_y_train)\n",
    "# my_y_pred = my_model.predict(my_X_test)\n",
    "# print(\"My Accuracy:\", metrics.accuracy_score(my_y_test, my_y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<details>\n",
    "  <summary>Click to see answer</summary>\n",
    "  <p align='left'>Are you sure you tried to solve it on your own?</p>\n",
    "          <code>\n",
    "              my_X_train, my_X_test, my_y_train, my_y_test = train_test_split(X, y, test_size=0.3)\n",
    "          </code>\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <a name=\"2.2\"></a>K-fold Cross-validation\n",
    "K-fold Cross-validation is probably the most common technique for model evaluation and model selection. \n",
    "- We split the dataset into *K* parts and iterate over a dataset set *K* times\n",
    "- In each round one part is used for validation, and the remaining *K-1* parts are merged into a training subset for model evaluation\n",
    "- We compute the cross-validation performance as the arithmetic mean over the *K* performance estimates from the validation sets.\n",
    "<img src=\"https://sebastianraschka.com/images/blog/2016/model-evaluation-selection-part3/kfold.png\" width=\"500\">\n",
    "\n",
    "##### Pros:\n",
    "    + Better estimate of out-of-sample performance than simple train/test split\n",
    "\n",
    "##### Cons:\n",
    "    - Runs \"K\" times slower than simple train/test split\n",
    "\n",
    "In the following example we will apply k-fold cross validation for Model Selection using *GridSearchCV* function.\n",
    "\n",
    "> #### GridSearchCV main parameters\n",
    ">*sklearn.model_selection.GridSearchCV*\n",
    "\n",
    ">**param_grid**: dict or list of dictionaries.\n",
    "Dictionary with parameters names (string) as keys and lists of parameter settings to try as values, or a list of such dictionaries, in which case the grids spanned by each dictionary in the list are explored. This enables searching over any sequence of parameter settings.\n",
    "\n",
    ">**cv**: int, cross-validation generator or an iterable, optional.\n",
    "Determines the cross-validation splitting strategy.\n",
    "\n",
    ">**scoring**: string, callable or None, default=None.\n",
    "Controls what metric to apply to the estimators evaluated\n",
    "\n",
    "[To the table of contents](#0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 4min 26s, sys: 5.86 s, total: 4min 32s\n",
      "Wall time: 4min 7s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "GridSearchCV(cv=3, error_score='raise',\n",
       "       estimator=Pipeline(steps=[('vectorizer', CountVectorizer(analyzer='word', binary=True, decode_error='strict',\n",
       "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
       "        lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
       "        ngram_range=(1, 1), preprocessor=None, stop_words=None,\n",
       "        ...ty='l2', random_state=None,\n",
       "          solver='liblinear', tol=0.0001, verbose=0, warm_start=False))]),\n",
       "       fit_params={}, iid=True, n_jobs=1,\n",
       "       param_grid={'classifier__C': [0.1, 0.01, 0.001]},\n",
       "       pre_dispatch='2*n_jobs', refit=True, return_train_score=True,\n",
       "       scoring=None, verbose=0)"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# fit model \n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "params = dict(classifier__C=[0.1, 0.01, 0.001])\n",
    "grid_search = GridSearchCV(pipeline, param_grid=params, cv=3)\n",
    "\n",
    "%time grid_search.fit(X_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'classifier__C': 0.1}"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Best parameters found:\n",
    "grid_search.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Accuracy 0.815501933032\n"
     ]
    }
   ],
   "source": [
    "# Average accuracy over K folds for best parameters set\n",
    "print(\"Validation Accuracy\", grid_search.best_score_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Accuracy: 0.795590994371\n"
     ]
    }
   ],
   "source": [
    "# Let's check how our model will generalize to unseen data\n",
    "df_new = pd.read_csv('../data/test.csv')\n",
    "X_test_new, y_test_new = df_new.text, df_new.label\n",
    "y_pred_new = grid_search.predict(X_test_new)\n",
    "\n",
    "print(\"Test Accuracy:\", metrics.accuracy_score(y_test_new, y_pred_new))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <a name=\"3\"></a>3. Classification metrics overview\n",
    "Classification problems are probably the most common type of ML problem and as such there are many metrics that can be used to evaluate predictions for these problems. We will review some of them.\n",
    "\n",
    "### <a name=\"3.1\"></a>Accuracy\n",
    "Accuracy simply measures *what percent of your predictions were correct*. It's the ratio between the number of correct predictions and the total number of predictions.\n",
    "\n",
    "$$accuracy = {\\frac{\\#\\ correct}{\\#\\ predictions}}$$\n",
    "\n",
    "[To the table of contents](#0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.819343424415\n"
     ]
    }
   ],
   "source": [
    "# calculate accuracy\n",
    "print(metrics.accuracy_score(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Accuracy is also the most misused metric. It is really **only suitable** when there are an *equal number of observations in each class* (which is rarely the case) and that all *predictions and prediction errors are equally important*, which is often not the case.\n",
    "\n",
    "### <a name=\"3.2\"></a>Confusion Matrix\n",
    "The confusion matrix is a handy presentation of the accuracy of a model with 2 or more classes. The table **presents predictions** on the x-axis and **accuracy outcomes** on the y-axis. The cells of the table are the number of predictions made by a machine learning algorithm."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[10013  2534]\n",
      " [ 2980 14995]]\n"
     ]
    }
   ],
   "source": [
    "# first argument is true values, second argument is predicted values\n",
    "# this produces a 2x2 numpy array (matrix)\n",
    "conf = metrics.confusion_matrix(y_test, y_pred)\n",
    "print(conf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "|                | Predicted Negative | Predicted Positive |\n",
    "|:--------------:|--------------------|--------------------|\n",
    "| **Negative Cases** |      TN: 9324      |      FP: 3266      |\n",
    "| **Positive Cases** |      FN: 2288      |      TP: 15644     |\n",
    "\n",
    "- **True Positives (TP)**:\n",
    "We correctly predicted that the reviews are positive: **15644**\n",
    "- **True Negatives (TN)**:\n",
    "We correctly predicted that the reviews are negative: **9324**\n",
    "- **False Positives (FP)**:\n",
    "We incorrectly predicted that the reviews are positive: **3266**\n",
    "- **False Negatives (FN)**:\n",
    "We incorrectly predicted that the reviews are negative: **2288**\n",
    "\n",
    "\n",
    "\n",
    "Confusion matrix allows you to compute various classification metrics, and these metrics can guide your model selection. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# slice confusion matrix into four pieces for future use\n",
    "TP = conf[1, 1]\n",
    "TN = conf[0, 0]\n",
    "FP = conf[0, 1]\n",
    "FN = conf[1, 0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can learn more about the [Confusion Matrix on the Wikipedia article](https://en.wikipedia.org/wiki/Confusion_matrix).\n",
    "\n",
    "[To the table of contents](#0)\n",
    "\n",
    "### <a name=\"3.3\"></a>Precision & Recall\n",
    "Precision and recall are actually two metrics. But they are often used together.\n",
    "\n",
    "**Precision** answers the question: *What percent of positive predictions were correct?*\n",
    "\n",
    "$$precision = {\\frac{\\#\\ true\\ positive}{\\#\\ true\\ positive + \\#\\ false\\ positive}}$$\n",
    "\n",
    "**Recall** answers the question: *What percent of the positive cases did you catch?*\n",
    "\n",
    "\n",
    "$$recall = {\\frac{\\#\\ true\\ positive}{\\#\\ true\\ positive + \\#\\ false\\ negative}}$$\n",
    "\n",
    "![](http://www.kdnuggets.com/images/precision-recall-relevant-selected.jpg)\n",
    "\n",
    "See also a very good explanation of [Precision and recall](https://en.wikipedia.org/wiki/Precision_and_recall) in Wikipedia.\n",
    "\n",
    "[To the table of contents](#0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.855439557305\n",
      "0.855439557305\n"
     ]
    }
   ],
   "source": [
    "# calculate precision\n",
    "precision = TP / float(TP + FP)\n",
    "\n",
    "print(precision)\n",
    "print(metrics.precision_score(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### <a name=\"3.3.1\"></a>Excercise 2: Compute Recall\n",
    "- Compute Recall metric in two different ways\n",
    "- Check if the two values are equal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# compute recall\n",
    "\n",
    "#### YOUR CODE STARTS HERE ###\n",
    "\n",
    "# recall = ...\n",
    "\n",
    "# print(recall)\n",
    "# print(...)\n",
    "\n",
    "#### YOUR CODE ENDS HERE ###"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<details>\n",
    "  <summary>Click to see answer</summary>\n",
    "  <p align='left'>Are you sure you tried to solve it on your own?</p>\n",
    "          <code>\n",
    "              recall = TP / float(FN + TP)\n",
    "              \n",
    "              print(recall)\n",
    "              print(metrics.recall_score(y_test, y_pred))\n",
    "          </code>\n",
    "\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <a name=\"3.4\"></a>F1-score\n",
    "The F1-score (sometimes known as the balanced F-beta score) is a single metric that combines both precision and recall via their harmonic mean:\n",
    "\n",
    "$$F_1 = 2 {\\frac{precision * recall}{precision + recall}}$$\n",
    "\n",
    "Unlike the arithmetic mean, the harmonic mean tends toward the smaller of the two elements. Hence the F1 score will be small if either precision or recall is small.\n",
    "\n",
    "[To the table of contents](#0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.844693555656\n",
      "0.844693555656\n"
     ]
    }
   ],
   "source": [
    "# calculate f1-score\n",
    "f1 = 2 * precision * recall / (precision + recall)\n",
    "\n",
    "print(f1)\n",
    "print(metrics.f1_score(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <a name=\"3.5\"></a>Classification Report\n",
    "Scikit-learn does provide a convenience report when working on classification problems to give you a quick idea of the accuracy of a model using a number of measures.\n",
    "\n",
    "The **classification_report()** function displays the precision, recall, f1-score and support for each class. (*support* is the number of occurrences of each class in *y_true*)\n",
    "\n",
    "[To the table of contents](#0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.77      0.80      0.78     12547\n",
      "          1       0.86      0.83      0.84     17975\n",
      "\n",
      "avg / total       0.82      0.82      0.82     30522\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# print a report on the binary classification problem\n",
    "print(metrics.classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <a name=\"4\"></a>4. Choice of Metrics\n",
    "Depending on your application, you may want to consider different performance metrics. Choice of metric depends on your business objective and on the data you have at hand.\n",
    "\n",
    "In many cases **accuracy** alone will be enough. It is suitable when the data is balanced (equal number of observations in each class) and when minimizing *False Positives* and *False Negatives* is equally important.\n",
    "\n",
    "If that is not the case:\n",
    "\n",
    "- Identify if FP or FN is more important to reduce\n",
    "- Choose metric with relevant variable (FP or FN) in the equation\n",
    "\n",
    "##### Case 1: Spam filter (positive class is \"spam\")\n",
    "FN (spam goes to the inbox) are more acceptable than FP (non-spam is caught by the spam filter) => Choose **FP** as a variable, optimize for **precision**\n",
    "\n",
    "##### Case 2: Fraudulent transaction detector (positive class is \"fraud\")\n",
    "FP (normal transactions that are flagged as possible fraud) are more acceptable than FN (fraudulent transactions that are not detected) => Choose **FN** as a variable, optimize for **recall**\n",
    "\n",
    "[To the table of contents](#0)\n",
    "\n",
    "---\n",
    "### <a name=\"5\"></a>Summary\n",
    "In this tutorial we briefly explored applications, techniques and metrics of model evaluation. We learned about 3 tasks of model evaluation:\n",
    "- Estimating model performance\n",
    "- Model selection\n",
    "- Algorithm selection\n",
    "\n",
    "2 common techniques:\n",
    "- Simple train/test split (Holdout validation)\n",
    "- K-fold cross-validation\n",
    "\n",
    "About 4 classification metrics:\n",
    "- Accuracy\n",
    "- Precision & Recall\n",
    "- F-1 score\n",
    "\n",
    "Also 2 convenience methods for classification prediction results:\n",
    "- Confusion matrix\n",
    "- Classification report\n",
    "\n",
    "And about choosing the right metric.\n",
    "\n",
    "#### Thank you!\n",
    "\n",
    "[To the table of contents](#0)\n",
    "\n",
    "---\n",
    "#### References\n",
    "- Sebastian Raschka, [Model evaluation, model selection, and algorithm selection in machine learning](https://sebastianraschka.com/blog/2016/model-evaluation-selection-part1.html)\n",
    "- Jason Brownlee, [Metrics To Evaluate Machine Learning Algorithms in Python](http://machinelearningmastery.com/metrics-evaluate-machine-learning-algorithms-python/)\n",
    "- Ritchie Ng, [Evaluating a Classification Model](http://www.ritchieng.com/machine-learning-evaluate-classification-model/)\n",
    "- [Turi Machine Learning Platform User Guide](https://turi.com/learn/userguide/evaluation/classification.html)\n",
    "- Gregory Piatetsky, [21 Must-Know Data Science Interview Questions and Answers](http://www.kdnuggets.com/2016/02/21-data-science-interview-questions-answers.html/2)"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda env:py35]",
   "language": "python",
   "name": "conda-env-py35-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
